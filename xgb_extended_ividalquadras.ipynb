{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../Data/bgsedsc_0.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Decision Trees\n",
    "### Ignacio Vidal-Quadras Costa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOSPITAL_EXPIRE_FLAG</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>HeartRate_Min</th>\n",
       "      <th>HeartRate_Max</th>\n",
       "      <th>HeartRate_Mean</th>\n",
       "      <th>SysBP_Min</th>\n",
       "      <th>SysBP_Max</th>\n",
       "      <th>SysBP_Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>LOS</th>\n",
       "      <th>age</th>\n",
       "      <th>weekday</th>\n",
       "      <th>WEEKDAY</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>hour</th>\n",
       "      <th>NIGHT</th>\n",
       "      <th>ETHNICS</th>\n",
       "      <th>num_diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>55440</td>\n",
       "      <td>195768</td>\n",
       "      <td>228357</td>\n",
       "      <td>89.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>121.043478</td>\n",
       "      <td>74.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>106.586957</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5761</td>\n",
       "      <td>69.608219</td>\n",
       "      <td>6</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>76908</td>\n",
       "      <td>126136</td>\n",
       "      <td>221004</td>\n",
       "      <td>63.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>79.117647</td>\n",
       "      <td>89.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>106.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7582</td>\n",
       "      <td>42.101370</td>\n",
       "      <td>0</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>95798</td>\n",
       "      <td>136645</td>\n",
       "      <td>296315</td>\n",
       "      <td>81.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.689655</td>\n",
       "      <td>88.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>112.785714</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7626</td>\n",
       "      <td>68.210959</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40708</td>\n",
       "      <td>102505</td>\n",
       "      <td>245557</td>\n",
       "      <td>76.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>98.857143</td>\n",
       "      <td>84.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>106.972973</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8734</td>\n",
       "      <td>74.961644</td>\n",
       "      <td>0</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28424</td>\n",
       "      <td>127337</td>\n",
       "      <td>225281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8654</td>\n",
       "      <td>79.424658</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HOSPITAL_EXPIRE_FLAG  subject_id  hadm_id  icustay_id  HeartRate_Min  \\\n",
       "0                     0       55440   195768      228357           89.0   \n",
       "1                     0       76908   126136      221004           63.0   \n",
       "2                     0       95798   136645      296315           81.0   \n",
       "3                     0       40708   102505      245557           76.0   \n",
       "4                     0       28424   127337      225281            NaN   \n",
       "\n",
       "   HeartRate_Max  HeartRate_Mean  SysBP_Min  SysBP_Max  SysBP_Mean  ...  \\\n",
       "0          145.0      121.043478       74.0      127.0  106.586957  ...   \n",
       "1          110.0       79.117647       89.0      121.0  106.733333  ...   \n",
       "2           98.0       91.689655       88.0      138.0  112.785714  ...   \n",
       "3          128.0       98.857143       84.0      135.0  106.972973  ...   \n",
       "4            NaN             NaN        NaN        NaN         NaN  ...   \n",
       "\n",
       "      LOS        age  weekday    WEEKDAY  SEASON      HOLIDAY  hour  NIGHT  \\\n",
       "0  4.5761  69.608219        6     Sunday  Summer  Non Holiday    10      0   \n",
       "1  0.7582  42.101370        0     Monday  Winter  Non Holiday    22      1   \n",
       "2  3.7626  68.210959        2  Wednesday  Autumn  Non Holiday    23      1   \n",
       "3  3.8734  74.961644        0     Monday  Autumn  Non Holiday     8      0   \n",
       "4  5.8654  79.424658        4     Friday  Autumn  Non Holiday     2      1   \n",
       "\n",
       "   ETHNICS  num_diag  \n",
       "0    OTHER        17  \n",
       "1    OTHER         3  \n",
       "2    OTHER        14  \n",
       "3    OTHER        39  \n",
       "4    OTHER        69  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "seed = 54\n",
    "# Training dataset\n",
    "data=pd.read_csv('pre_processed_data.csv')\n",
    "data = data.drop('Unnamed: 0', axis =1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-processed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>HeartRate_Min</th>\n",
       "      <th>HeartRate_Max</th>\n",
       "      <th>HeartRate_Mean</th>\n",
       "      <th>SysBP_Min</th>\n",
       "      <th>SysBP_Max</th>\n",
       "      <th>SysBP_Mean</th>\n",
       "      <th>DiasBP_Min</th>\n",
       "      <th>...</th>\n",
       "      <th>FIRST_CAREUNIT</th>\n",
       "      <th>age</th>\n",
       "      <th>weekday</th>\n",
       "      <th>WEEKDAY</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>HOLIDAY</th>\n",
       "      <th>hour</th>\n",
       "      <th>NIGHT</th>\n",
       "      <th>ETHNICS</th>\n",
       "      <th>num_diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>93535</td>\n",
       "      <td>121562</td>\n",
       "      <td>200011</td>\n",
       "      <td>56.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>71.205128</td>\n",
       "      <td>123.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>156.411765</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>MICU</td>\n",
       "      <td>84.287671</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>30375</td>\n",
       "      <td>177945</td>\n",
       "      <td>200044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>SICU</td>\n",
       "      <td>81.964384</td>\n",
       "      <td>6</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>73241</td>\n",
       "      <td>149216</td>\n",
       "      <td>200049</td>\n",
       "      <td>54.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>64.833333</td>\n",
       "      <td>95.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>114.545455</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>MICU</td>\n",
       "      <td>64.441096</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>99052</td>\n",
       "      <td>129142</td>\n",
       "      <td>200063</td>\n",
       "      <td>85.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>92.560976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>108.365854</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>CSRU</td>\n",
       "      <td>37.095890</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>51698</td>\n",
       "      <td>190004</td>\n",
       "      <td>200081</td>\n",
       "      <td>82.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>94.323529</td>\n",
       "      <td>86.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>111.093750</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>CCU</td>\n",
       "      <td>69.920548</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Non Holiday</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id  hadm_id  icustay_id  HeartRate_Min  HeartRate_Max  \\\n",
       "4930       93535   121562      200011           56.0           82.0   \n",
       "1052       30375   177945      200044            NaN            NaN   \n",
       "3412       73241   149216      200049           54.0           76.0   \n",
       "1725       99052   129142      200063           85.0          102.0   \n",
       "981        51698   190004      200081           82.0          133.0   \n",
       "\n",
       "      HeartRate_Mean  SysBP_Min  SysBP_Max  SysBP_Mean  DiasBP_Min  ...  \\\n",
       "4930       71.205128      123.0      185.0  156.411765        37.0  ...   \n",
       "1052             NaN        NaN        NaN         NaN         NaN  ...   \n",
       "3412       64.833333       95.0      167.0  114.545455        33.0  ...   \n",
       "1725       92.560976       91.0      131.0  108.365854        42.0  ...   \n",
       "981        94.323529       86.0      143.0  111.093750        47.0  ...   \n",
       "\n",
       "      FIRST_CAREUNIT        age  weekday   WEEKDAY  SEASON      HOLIDAY  hour  \\\n",
       "4930            MICU  84.287671        3  Thursday  Winter  Non Holiday    20   \n",
       "1052            SICU  81.964384        6    Sunday  Winter  Non Holiday    16   \n",
       "3412            MICU  64.441096        5  Saturday  Winter  Non Holiday    22   \n",
       "1725            CSRU  37.095890        1   Tuesday  Summer  Non Holiday    23   \n",
       "981              CCU  69.920548        3  Thursday  Summer  Non Holiday     6   \n",
       "\n",
       "      NIGHT  ETHNICS  num_diag  \n",
       "4930      1    OTHER        39  \n",
       "1052      0    OTHER         5  \n",
       "3412      1    OTHER        25  \n",
       "1725      1    OTHER        18  \n",
       "981       1    OTHER        24  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset (to produce predictions)\n",
    "data_test=pd.read_csv('pre_processed_data_test.csv')\n",
    "data_test = data_test.drop('Unnamed: 0', axis =1)\n",
    "data_test.sort_values('icustay_id').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import scipy\n",
    "import datetime\n",
    "import imblearn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection  import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlens.ensemble import SuperLearner, BlendEnsemble, Subsemble, SequentialEnsemble\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(3123) # impose random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiaction of our variables and categorical transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiaction of variables\n",
    "I will eliminate DOD, DISCHTIME because when the patient enters the hospital you cannot know this information (he is still alive)\n",
    "\n",
    "Subject_id, hadm_id, icustay_id are eliminated because they have no explanatory power\n",
    "\n",
    "DIAGNOSIS will be droped as I will use the ICD9_diagnosis (because it has fewer observations)\n",
    "\n",
    "LOS is not included because you do not know it when you get into the hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['LOS']\n",
    "\n",
    "numericals = ['HeartRate_Min','HeartRate_Max','HeartRate_Mean','SysBP_Min',\n",
    "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
    "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
    "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
    "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
    "       'Glucose_Mean', 'age', 'com_prob', 'num_diag']\n",
    "\n",
    "\n",
    "categoricals = ['ADMISSION_TYPE', 'FIRST_CAREUNIT', 'GENDER', 'ETHNICS',\n",
    "                'RELIGION', 'INSURANCE','NIGHT', 'WEEKDAY', 'SEASON', 'DIAG', 'HOLIDAY']\n",
    "#Notice DIAG and com_prob do not exist yet, they will be added in the following steps\n",
    "\n",
    "variables = numericals + categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding - Target encoding\n",
    "I will encode the categoricals using target encoding. I have tried creating dummies but, besides being much slower, the algorythm works worse.\n",
    "\n",
    "The target encoding is not done weith the target variable (in this case LOS) but with the probability of death, so it is not a proper target encoding but they are very similar as the more likely the client is to die, the more severe the illness is and the longer he will stay in the hospital.\n",
    "\n",
    "Notice that on this code we don't use the same procedure for ICD9_diagnosis. This is because it might be the case that the diagnosis in the test sample is a new one (does not exist in the train set), in this case I will assign to this variable the general progability of dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [i for i in categoricals if i != 'DIAG']:\n",
    "    dict = data.groupby(n)['HOSPITAL_EXPIRE_FLAG'].mean().to_dict()  \n",
    "    data[n] = data[n].apply(lambda x: dict[x])\n",
    "    data_test[n] = data_test[n].apply(lambda x: dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_mean_dict = data.groupby('ICD9_diagnosis')['HOSPITAL_EXPIRE_FLAG'].mean().to_dict()\n",
    "\n",
    "def get_diag(x):\n",
    "    if x in diag_mean_dict:\n",
    "        return diag_mean_dict[x]\n",
    "    else: \n",
    "        return data.HOSPITAL_EXPIRE_FLAG.mean()\n",
    "\n",
    "data['DIAG'] = data['ICD9_diagnosis'].apply(get_diag)\n",
    "data_test['DIAG'] = data_test['ICD9_diagnosis'].apply(get_diag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commulative probabilty per patient based on diagnosis\n",
    "One patient can have come before to the hospital so he was already assigned a probability of death. In this variable we exactly capture that, if the patient has come a frist time and according to his diagnosis he was assigned a death probability of 0.01, then if he goes again and according to the diagnosis he has a probability of dying of 0.1, then the com_prob variable will account for the 0.11 commulative probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['com_prob'] = data.sort_values(['subject_id', 'ADMITTIME']).groupby('subject_id')['DIAG'].cumsum()\n",
    "\n",
    "data_test['com_prob']=data_test.sort_values(['subject_id', 'ADMITTIME']).groupby('subject_id')['DIAG'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select from the train and test only the variables we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[variables]\n",
    "X_test = data_test[variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make sure that both datasets have the same columns so that we can apply KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.isin(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
       "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
       "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
       "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
       "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
       "       'Glucose_Mean', 'age', 'com_prob', 'num_diag', 'ADMISSION_TYPE',\n",
       "       'FIRST_CAREUNIT', 'GENDER', 'ETHNICS', 'RELIGION', 'INSURANCE', 'NIGHT',\n",
       "       'WEEKDAY', 'SEASON', 'DIAG', 'HOLIDAY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_use = X_train.columns[X_train.columns.isin(X_test.columns)]\n",
    "len(col_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5221, 38)\n",
      "(20885, 38)\n"
     ]
    }
   ],
   "source": [
    "col_to_use = X_train.columns[X_train.columns.isin(X_test.columns)]\n",
    "len(col_to_use)\n",
    "X_train = X_train[col_to_use]\n",
    "X_test = X_test[col_to_use]\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous in this assigment I tried different ways of handeling missing data.\n",
    "\n",
    "I tried deleating all the rows with at least one missing value in the train data set and substituting the missing values in the test with the median of the varibale but my accuracy went down a lot.\n",
    "\n",
    "Then I also tried the with the KNN method for missing data.\n",
    "\n",
    "I also tried to subsitute them by just the mean.\n",
    "\n",
    "Finally, the median turned out to be the most efficient one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(X_train.median(), inplace=True)\n",
    "X_test.fillna(X_test.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engenieering\n",
    "##### (Find the code in the support_file)\n",
    "\n",
    "I will not explore the skewness and kurtosis of the numerical varibales because the models I am implementing are Â¡ and XGBoost and random forest, since decision trees do not require normalization of their inputs and both random forests and XGBoost are a variation of decision trees, they do not require normalization of their imnputs either. As consequence I will not apply logs or any other normalization technique.\n",
    "\n",
    "I have droped categorical data so I will note use feature slips.\n",
    "\n",
    "I tried to delete records with ourliers from the data set but the outcome was not good. Defined as below 0.1 quantile and above 0.9 quantile.\n",
    "\n",
    "For varibale reduction I explored at the correlation matrix and tried to delete some correlated varibales but did not end up beeing helpul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the predictions with the grid search in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE::-22.74404293221945\n",
      "Best Hyperparameters::\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'random_state': 54, 'sampling_method': 'uniform'}\n",
      "CPU times: user 1min 15s, sys: 122 ms, total: 1min 15s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "grid_values = {\n",
    "    'n_estimators' : [100,200],\n",
    "    'learning_rate': [0.1], \n",
    "    'max_depth': [3],\n",
    "    'random_state' : [seed],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "\n",
    "grid_xgb_acc = GridSearchCV(model, param_grid = grid_values, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_xgb_acc.fit(X_train, y)\n",
    "\n",
    "y_pred_acc = grid_xgb_acc.predict(X_test)\n",
    "\n",
    "print(\"MSE::{}\".format(grid_xgb_acc.best_score_))\n",
    "print(\"Best Hyperparameters::\\n{}\".format(grid_xgb_acc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_diag          1.000000\n",
       "TempC_Max         0.762976\n",
       "DIAG              0.642572\n",
       "Glucose_Max       0.618550\n",
       "HeartRate_Max     0.530426\n",
       "RespRate_Max      0.488970\n",
       "SysBP_Min         0.441329\n",
       "SpO2_Max          0.436577\n",
       "FIRST_CAREUNIT    0.407280\n",
       "DiasBP_Min        0.374537\n",
       "TempC_Mean        0.320553\n",
       "SpO2_Min          0.304181\n",
       "SysBP_Max         0.280317\n",
       "RespRate_Mean     0.275251\n",
       "Glucose_Min       0.271678\n",
       "SpO2_Mean         0.266647\n",
       "age               0.262331\n",
       "MeanBP_Max        0.262001\n",
       "TempC_Min         0.249088\n",
       "SysBP_Mean        0.236052\n",
       "com_prob          0.229403\n",
       "MeanBP_Min        0.221853\n",
       "DiasBP_Mean       0.221654\n",
       "WEEKDAY           0.214883\n",
       "MeanBP_Mean       0.212866\n",
       "Glucose_Mean      0.211198\n",
       "DiasBP_Max        0.196942\n",
       "HeartRate_Min     0.193808\n",
       "HeartRate_Mean    0.183663\n",
       "INSURANCE         0.176269\n",
       "NIGHT             0.167249\n",
       "RespRate_Min      0.164482\n",
       "ADMISSION_TYPE    0.138974\n",
       "RELIGION          0.089468\n",
       "GENDER            0.087909\n",
       "ETHNICS           0.000000\n",
       "SEASON            0.000000\n",
       "HOLIDAY           0.000000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = grid_xgb_acc.best_estimator_.feature_importances_\n",
    "important_features = pd.Series(data=importances/importances.max() ,index=X_train.columns).sort_values(ascending=False)\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the importances I will drop the ones with importance below 0.05, which are the ones that have no importance on the model. Season, Holiday and Ethincs.\n",
    "After this I will run again XGBoost with the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['SEASON', 'HOLIDAY', 'ETHNICS'], axis = 1)\n",
    "X_test = X_test.drop(['SEASON', 'HOLIDAY', 'ETHNICS'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE::-22.709509039589673\n",
      "Best Hyperparameters::\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'random_state': 54, 'sampling_method': 'uniform'}\n",
      "CPU times: user 1min 19s, sys: 121 ms, total: 1min 19s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "grid_values = {\n",
    "    'n_estimators' : [100,200],\n",
    "    'learning_rate': [0.1], \n",
    "    'max_depth': [3],\n",
    "    'random_state' : [seed],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "\n",
    "grid_xgb_acc = GridSearchCV(model, param_grid = grid_values, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_xgb_acc.fit(X_train, y)\n",
    "\n",
    "y_pred_acc = grid_xgb_acc.predict(X_test)\n",
    "\n",
    "print(\"MSE::{}\".format(grid_xgb_acc.best_score_))\n",
    "print(\"Best Hyperparameters::\\n{}\".format(grid_xgb_acc.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, after having tried differnt approaches to the model applying the models and techniques we have learned in class, the XGBoost after having selected these variables with an importance abpve 0.05 ended up to be the best one for predicitons. This XGBoost is using 200 number of gradient boos trees, a boosting learning rate of 0.1 and a maximium tree depth of 3, and where each training instance has the same probability of being selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(data_test['icustay_id'].copy())\n",
    "pred_df['LOS'] = y_pred_acc\n",
    "pred_df.to_csv('XGB_Prediction_kaggle', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
